{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will mount your own Google Drive and change the working directory."
      ],
      "metadata": {
        "id": "-_ZkNxqGGhdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DWQh-lq8GuwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab79e118-c872-44ce-deb1-29d7e7cd7e74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "%ls\n",
        "%cd /content/drive/MyDrive/colabhw/"
      ],
      "metadata": {
        "id": "P_5Tf1rMHBQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6886e2dc-93c6-4d32-b5ca-000c29796b54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "/content/drive/MyDrive/colabhw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5JywoPOO_Oll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54d5e11-6b35-4880-f57f-f4af4f1ad1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m199.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.14.1)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.7.14)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.7.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.23.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.2 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.25.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.2.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.2 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n",
            "--2025-07-20 13:13:41--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.121, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250720%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250720T130410Z&X-Amz-Expires=3600&X-Amz-Signature=350b4db55cff07aba6e964ddb083c99be3fcac64df0aff93a2ba776992182343&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1753020250&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzAyMDI1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=javq16lJNzF33C5yllquu0zSWzElwZ2UhcqK2VI0K470v2pzEhUZhL-bXvk%7EzFzZ4AIwRj2ox9iEK0FXsT72UnrLgmJeDpdHctmaUvk2oQ2Zlc7jcv6Uvh7ASP2vcd8m9MpLzBn5-cAa7wbYBVd1%7Ev6Wa9Harz-WjbOKTTHC3-LpVbf01zFQrgoCaQxd4iSvPHJOQncAkt5CN73FHPosXtx3lvLYhCsukn7UB-9%7EacY35cHJC0myK2EzTOrc3Kb6UMDKzogwU2VPqTMP7ckShcBsNGh1IWcDaSBDzQdJkngtgn1kOSzmuu3AR9cc4x%7E97QRSz8x2-71XgFN5Cydthg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-07-20 13:13:41--  https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250720%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250720T130410Z&X-Amz-Expires=3600&X-Amz-Signature=350b4db55cff07aba6e964ddb083c99be3fcac64df0aff93a2ba776992182343&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1753020250&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzAyMDI1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=javq16lJNzF33C5yllquu0zSWzElwZ2UhcqK2VI0K470v2pzEhUZhL-bXvk%7EzFzZ4AIwRj2ox9iEK0FXsT72UnrLgmJeDpdHctmaUvk2oQ2Zlc7jcv6Uvh7ASP2vcd8m9MpLzBn5-cAa7wbYBVd1%7Ev6Wa9Harz-WjbOKTTHC3-LpVbf01zFQrgoCaQxd4iSvPHJOQncAkt5CN73FHPosXtx3lvLYhCsukn7UB-9%7EacY35cHJC0myK2EzTOrc3Kb6UMDKzogwU2VPqTMP7ckShcBsNGh1IWcDaSBDzQdJkngtgn1kOSzmuu3AR9cc4x%7E97QRSz8x2-71XgFN5Cydthg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.125, 18.155.68.69, 18.155.68.14, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8540775840 (8.0G)\n",
            "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
            "\n",
            "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G  72.0MB/s    in 2m 1s   \n",
            "\n",
            "2025-07-20 13:15:42 (67.6 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
            "\n",
            "--2025-07-20 13:15:42--  https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4399 (4.3K) [text/plain]\n",
            "Saving to: ‘public.txt’\n",
            "\n",
            "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-20 13:15:42 (123 MB/s) - ‘public.txt’ saved [4399/4399]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kX6SizAt_Olm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e932c6-a103-425e-deee-e026f03b199d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ScyW45N__Olm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822f4492-980d-44b8-9bf0-70871bad59fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test Code by use the search function\n",
        "results = await search(\"牛顿 万有引力定律\", n_results=3)\n",
        "for i, txt in enumerate(results, 1):\n",
        "    print(f\"--- 结果 {i} ---\")\n",
        "    print(txt[:120], \"...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsE2-UBJ_Im5",
        "outputId": "54fb051e-4764-4525-e2a6-ce07633df12e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 结果 1 ---\n",
            "牛顿万有引力定律-维基百科，自由的百科全书跳转到内容主菜单主菜单移至侧栏隐藏导航首页分类索引特色内容新闻动态最近更改随机条目帮助帮助维基社群方针与指引互助客栈知识问答字词转换IRC即时聊天联络我们关于维基百科特殊页面搜索搜索外观资助维基百科 ...\n",
            "\n",
            "--- 结果 2 ---\n",
            "3.3:牛顿万有引力定律-GlobalSkiptomaincontentTogglesTableofContentsMenumenusearchSearchbuild_circleToolsfact_checkHomeworkcancelE ...\n",
            "\n",
            "--- 结果 3 ---\n",
            "视频:牛顿万有引力定律JoVEJoVE科研行为学生物化学生物学生物工程癌症研究化学发育生物学工程学环境科学遗传学免疫与感染医学神经科学JoVE杂志JoVE实验百科全书JoVEChromeExtension教育生物学化学Clinical工程学 ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8dmGCARd_Oln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58004ce6-90e9-4a5d-e4df-34af706fd0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**中文答案：**\n",
            "\n",
            "泰勒·斯威夫特（Taylor Swift）是一位美国歌手、词曲作家和音乐制作人。她以她的情感丰富的流行摇滚乐风格而闻名。出生于1989年，Swift从小就对演唱会有着深厚的情怀，她在10岁时开始写自己的第一首单独创意歌谣，并且她也曾经是美国乡村音乐电视台（CMT）的一位常客。\n",
            "\n",
            "斯威夫特的职业生涯始于2005年，当她的第一个专辑《泰勒·史维芙》发行。然而，她真正走红是在2010年的第二张專輯「Fearless」之后，該張唱片獲得了多項獎项，並且她也因此成为第一位在20岁之前获得两座格莱美奖的歌手。\n",
            "\n",
            "**日语答案：**\n",
            "\n",
            "テイラー・スウィフト（Taylor Swift）は、アメリカのシンガーソングライター、作曲家そして音楽プロデュースャーである。彼女は感情豊かなポップロックサウンドで知られており、そのキャリアでは数多くのヒットを生み出している。\n",
            "\n",
            "1989年に誕生物まれ後すぐに歌手としての道へ進むとともに関西地方などでも活動し、10歳から最初の一曲を作り始めていた。彼女はCMT（カントリー・ミュージックテレビジョン）で活躍することもあった。そのキャリアを通して数多くの賞を受けている。\n",
            "\n",
            "2010年にリードシングル「白い狼」がヒットし、デビューから5年目にして初めてのアルバム『フレイヤー』は大きな成功をおさめ、その後も彼女の人気と影響力だけではなく音楽的にも成長を続けており、今では世界中で有名なアーティストの一人となっている。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='who is Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文和日语两种语言來回答問題，生成这两种语言的答案。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一个专业的问题分析专家，擅长从复杂问题中提取核心内容。\",\n",
        "    task_description=\"请分析以下问题，提取核心问题内容，保持问题的核心含义。如果问题已经简洁明了，请直接返回原问题。\",\n",
        ")\n",
        "\n",
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一个专业的关键词提取专家，擅长从问题中识别最重要的搜索关键词。\",\n",
        "    task_description=\"请从以下问题中提取3-5个最重要的搜索关键词，用空格分隔。关键词应该能准确反映问题的核心内容，便于网络搜索找到相关信息。\",\n",
        ")\n",
        "\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。\",\n",
        "    task_description=\"请根据你的知识库和提供的相关信息，全面、准确地回答以下问题。如果相关信息与问题相关，请充分利用；如果信息不足，请基于你的知识进行回答。直接输出最终答案的内容。不用输出中间的分析过程和不相关的信息。\",\n",
        ")\n",
        "\n",
        "answer_verification_agent = LLMAgent(\n",
        "    role_description=\"你是一个专业的答案检查专家。你的任务是对于被提出的问题，确保AI生成的答案是准确的,符合逻辑的。\",\n",
        "    #task_description=\"请仔细检查以下答案是否正确，如果有错误请修正答案，并直接输出最终修正答案的内容，如果没有错误，直接输出最终答案的内容。\",\n",
        "    task_description=\"请检查核对答案是否正确的符合逻辑的回答了被提出的问题，如果有错误请修正答案，并直接输出最终修正答案的内容，如果没有错误，直接输出最终答案的内容。不用输出中间的分析过程和不相关的信息。\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
        "\n",
        "    # Step 1: 问题清理\n",
        "    cleaned_question = question_extraction_agent.inference(question);\n",
        "    #print(cleaned_question)\n",
        "    # Step 2: 关键词提取\n",
        "    keywords = keyword_extraction_agent.inference(cleaned_question);\n",
        "    #print(keywords)\n",
        "\n",
        "    # Step 3: 网络搜索\n",
        "    # Reduce the number of search results or truncate the content to fit within the context window\n",
        "    search_results = await search(keywords, n_results=3); # Reduce n_results to 3 as a starting point\n",
        "    # Truncate each search result to a reasonable length, e.g., 2000 characters\n",
        "    truncated_search_results = [result[:2000] for result in search_results];\n",
        "    enhanced_question = f\"问题：{cleaned_question}\\n\\n相关信息：{' '.join(truncated_search_results)}\";\n",
        "    #print(enhanced_question)\n",
        "    # Step 4: 生成初步答案\n",
        "    initial_answer = qa_agent.inference(enhanced_question);\n",
        "    #print(initial_answer)\n",
        "\n",
        "    # Step 5: 答案质量检查;\n",
        "    #final_answer = answer_verification_agent.inference(initial_answer);\n",
        "    #print(final_answer)\n",
        "\n",
        "    #return final_answer\n",
        "    return initial_answer"
      ],
      "metadata": {
        "id": "V6gQoFtT7Qm5"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test -Agent system for 1 message\n",
        "test_question1 = \"校歌為學校（包括小學、中學、大學等）宣告或者規定的代表該校的歌曲。用於體現該校的治學理念、辦學理想等學校文化。「虎山雄風飛揚」是哪間學校的校歌歌詞？,光華國小\"\n",
        "answer1 = await pipeline(test_question1)\n",
        "answer1 = answer.replace('\\n',' ')\n",
        "print(answer1)"
      ],
      "metadata": {
        "id": "HipCcColHkfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer the questions using your pipeline!"
      ],
      "metadata": {
        "id": "P_kI_9EGB0S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ],
      "metadata": {
        "id": "PN17sSZ8DUg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"li4agent_answer22\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "plUDRTi_B39S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a170830-e0bf-4000-d8a0-a1afa81e4b16"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"虎山雄風飛揚 \"是台北市立中學的校歌。\n",
            "2 我無法提供相關的資訊。\n",
            "3 第一代 iPhone 是由史蒂夫·乔布斯（Steve Jobs）發表。\n",
            "4 根據提供的資訊，托福網路測驗（TOEFL iBT）總分為120 分。考生需要達到一定得數才能申請進階英文免修。  一般而言，大多数学校对 TOFEL 考试成绩有以下要求：  *   C1 级别：90-110     *  这意味着你能理解复杂的学术文章，包括大量信息和困难词汇。              在听力方面，你可以了解在大学环境中进行的一般性对话或演讲，并且能够识出主要观点、细节以及说话者的态度。  *   B2 级别：70-89     *  这意味着你能理解大多数学术文章的主旨和重要信息，但可能会遇到一些困难。              在听力方面，你可以了解在大学环境中进行的一般性对话或演讲，并且能够识出主要观点、细节以及说话者的态度。  *   B1 级别：50-69     *  这意味着你能理解简单的学术文章，但可能会遇到一些困难。              在听力方面，你可以了解在大学环境中进行的一般性对话或演讲，并且能够识出主要观点、细节以及说话者的态度。  *   A2 级别：30-49     *  这意味着你可能会遇到一些困难。              在听力方面，你可以了解在大学环境中进行的一般性对话或演讲，并且能够识出主要观点、细节以及说话者的态度。  *   A1 级别：0-29     *  这意味着你可能会遇到一些困难。              在听力方面，你可以了解在大学环境中进行的一般性对话或演讲，并且能够识出主要观点、细节以及说话者的态度。  因此，考生需要達到的托福網路測驗（TOEFL iBT）總分為90-110才能申請進階英文免修。\n",
            "5 在Rugby Union中，觸地try可得7分。\n",
            "6 我找不到相關的信息。\n",
            "7 熊仔的碩士指導教授為李琳山。\n",
            "8 詹姆斯·克拉ーク・マ克斯韦尔。\n",
            "9 距離國立臺灣史前文化博物館最近的臺鐵車站為康樂駅。\n",
            "10 20+30=50\n",
            "11 洛杉矶湖人队\n",
            "12 由于相关信息中没有提到2024年美国总统大选的胜出者，因此无法提供准确答案。\n",
            "13 Llama3.2系列中，參數量最小的模型是1B参数。\n",
            "14 依據國立臺灣大學學則，沒有報告書的情況下，每個学生每年至多可以停止5門課程。\n",
            "15 DeepSeek的母公司是幻方量化。\n",
            "16 我没有找到相关的信息来回答这个问题。\n",
            "17 這種特殊結構稱為「橋樑」（Bridge）或是 \"bridgehead\"。\n",
            "18 阿兰·图灵是被誉为“计算机科学之父”的人，他提出圖靈機概念，为現代計算理論奠定了基礎。\n",
            "19 南投縣名間鄉\n",
            "20 Windows 作業系統是微軟公司的產品。\n",
            "21 官將首起源自新北市的「地藏庵」。\n",
            "22 《咒》的邪神名為大黑佛母。\n",
            "23 這句台詞是「短暫交會就此分岔」的問題，核心內涵為：找出哪一首歌曲的主題與之相符。\n",
            "24 2025 卑南族聯合年聚在卞本部落舉辦。\n",
            "25 GeForce RTX 40系列是NVIDIA发布的图形处理器（GPU）产品，接替了前一代 GeForceRT X30 系列。该家族基于爱达·勒芙蕾丝微架构，并支持第三 代光线追踪功能。  首批公布的是三款 GPU：GeForce RTX 4090、 GeForec e R T x408080（16GB）和R TX40系列的特性包括：  *   CUDA计算能力8.9 *TSMC4N制程* 第四代张量核心 * 第三 代光线追踪核心 *   GeForceRT X40600000年6月29日上市，价格为299美元。该 GPU 配备 30 SM、3840 个CUDA内存单元和48 GB GDDR6128115的显卡。  GeForce RTX40702023 年4 月13 日发布，该GPU配有35.8亿个晶体管核心面积约18平方毫米，支持PCIe x16接口。该 GPU 的频率为 1,680 MHz、内存带宽达211250 GB/s，并且采用GDDR6X显卡。  GeForce RT X40902022 年10 月12 日发布，该GPU配有76.3亿个晶体管核心面积约754平方毫米，支持PCIe x16接口。该 GPU 的频率为 1,680 MHz、内存带宽达211008450 GB/s，并且采用GDDR6X显卡。  GeForce RT X4090D2024 年2 月28 日发布，该GPU配有76.3亿个晶体管核心面积约754平方毫米，支持PCIe x16接口。该 GPU 的频率为 1,680 MHz、内存带宽达212410083844 GB/s，并且采用GDDR6X显卡。  GeForce RT X40802022 年11 月17 日发布，该GPU配有45.9亿个晶体管核心面积约324平方毫米，支持PCIe x16接口。该 GPU 的频率为 1,680 MHz、内存带宽达220 GB/s，并且采用GDDR6显卡。  GeForce RT X40702023 年4 月13 日发布，该GPU配有35.8亿个晶体管核心面积约18平方毫米，支持PCIe x16接口。该\n",
            "26 大S是在日本旅游期间因流感并发肺炎而猝逕去世，享年48岁。\n",
            "27 艾萨克·牛顿。\n",
            "28 TAIHUCAIS\n",
            "29 \"I'll be back\" 是出自 1984 年电影《终结者》的经典台词。\n",
            "30 水的化學式為H2O。\n",
            "31 第 15 個作業名稱是什麼？  答案：無法從提供的資訊中找到相關信息。\n",
            "32 目前臺灣多數獨立學院皆已升格為大學，公 立的 獨립 學院僅剩一間，即國防醫學研究中心所屬之 國 防 醫 會。\n",
            "33 BitTorrent 协议运用了种子文件和Tracker 服务器的机制来确保当一个新的节点加入网络时，尚无任何 chunk 时，也能从其他 seed 隔离地获得部分数据，以利于后续整个网路 的 数据交换。  具体来说，当新 nodes 加入到 BitTorrent 网络中，它们首先会连接 Tracker 服务器以获取当前下载某个文件的所有 peer（节点）的 IP 和端口信息。然后，新的 node 会随机地从其他 seed 节点处获得部分数据块，从而开始进行上传和 下载。  这种设计使得新加入网络中的 nodes 可以来自不同源头的地方得到所需资源，这样可以提高下载速度并减少对单个节点的依赖。此外，由于每一个 node 都是同时作为服务器提供服务，同时也在从其他人处获得数据，因此 BitTorrent 网络能够更好地利用带宽和处理能力，从而实现高效、可靠 的文件共享。\n",
            "34 我无法找到相关信息。\n",
            "35 戈芬氏鳳頭鸚鵡偏好的奶油品項是什麼？\n",
            "36 桃園Xpark國王企鵝寶宝叫Tomorin。\n",
            "37 根據我的知識庫，國立臺灣大學的物理治療學系一般修業年限為四年的。\n",
            "38 我找不到相关信息。\n",
            "39 日本戰國時代被稱為「甲斐之虎」的人物是武田信玄。\n",
            "40 王肥貓可以考慮選修以下幾門課程：  1. 生物的演化與絕滅／魏國彥：這是一堂很和平也能學到東西的心理健康教育，老師是個溫文爾雅的人。 2..跨越時空談宇宙 ／陳丙燊: 進入課程後會有點壓力，但上课本身非常開心的事 3. 荀子哲学：东亚伦治之基礎／佐藤将人 : 这门课程对王肥貓来说可能是很吸引人的，因为老師讲解得详细，讓他更深刻了解那段时间的历史和學術思想發展。 4..海洋环境概論 ／范光龙: 這門課程跟形式非常科普，对於一所集合了一群「會考試」的人們而言，可掌握度很重要 5. 臺灣歷史與文化／呂紹理：這堂课对王肥貓来说可能是比較容易的，因为老師授教結合更多案例和历史学术，人也比较好。 6..中國繪畫學習 ／林聖智: 這門課程對於特別有興趣的人來說，可以了解各朝代的情況\n",
            "41 极限体能王SASUKE是日本TBS电视台不定期播出的运动娱乐特别节目。正式名称为“终極的生存游戏 S AS U KE”。该節目的名稱源自猿飞佐助，第1回是在1997年秋天播放，当初作为《挑战冠军王》中的特別單元形式放送。但由於SASUKE深受观众喜爱，从2002年的10届大会起以独立节目型态继续播出。每次節目的开场白中，主持人古馆伊知郎总是会说“选拔出的100名精英挑战超极限的生存攻击！勇者们，现在就成为伟大的先辈吧! 来到钢铁魔城顶点...”。这句话成为了本节目的重要象征。\n",
            "42 根據史料記載，出身於利嘉部落後來成為初鹿頭目的漢人名叫張英。\n",
            "43 《BanG Dream! Ave Mujica》的片頭曲是 《黒のバースデイ》。\n",
            "44 Linux作業系統最早於1991年首次發布。\n",
            "45 Likavung 的中文名稱為 Likawang。\n",
            "46 紅茶是全發酵的tea類。\n",
            "47 真紅眼黑龍與「黒魔導」（Black Magic）作為融合素材的 融 合怪獸是超級巨人。\n",
            "48 豐田萌繪在《BanG Dream!》企劃中，擔任松原花音的聲優。\n",
            "49 Rugby Union 中，9 號球員的正式名稱為 scrum-half。\n",
            "50 海王星。\n",
            "51 臺灣最早成立的野生動物救傷單位位於宜蘭縣。\n",
            "52 農業部生物多樣性研究所\n",
            "53 DeSTA2: Developing Instruction-Following Speech Language Model Without speech instruction-tuning data  该研究提出了一种简单有效的自动过程，用于创建包含语音 parole 语言理解能力而保留文本基准 LLM 原有言论特性的speech-text对数据。这种方法可以在不需要额外的大量注释和风险丢失原始文字功能的情况下使SLM具有广泛性，以适应与任务相关的动态SUPERB AIR-Bench-Chat等评估标准取得了显著表现。此模型还能够遵循来自LLMs复杂指令，例如特定输出格式化以及链式推理。\n",
            "54 太陽系中體積最大的行星是木衛。\n",
            "55 其中一族的話音與其他话声一般不被視為同群的是達悟語。\n",
            "56 这个问题已经简洁明了，核心含义是询问讲出这句话的老師身份。\n",
            "57 太魯閣族的打招呼用語是「embiyax namu kana」。\n",
            "58 我無法找到與「鄄族、布农侨息 息」相關的信息。\n",
            "59 女主角隱藏的是冒險者身份。\n",
            "60 根据卑南族的传说，姊弟Tuku及Sihasihau分别创建了两个部落，其中姐妹Sister TUKU创造了一支名为\"阿拉鲁\"(Alaru)或是 \"Aralu”的人類。\n",
            "61 KO榜是《終極一班》中的一個重要設定，代表著角色戰力的排名。根據劇情發展和人物的變化，這些高手們現況又如何呢？以下為大家整理這一些熟悉面孔最新動態：  1. KO136鯊魚（謝 和弦飾）：已經結婚並育有兩名女兒，事業與家庭雙豐收。 2.KO13煞姐 （黃小柔 飾） : 依然活躍於演藝圈，不僅以歌手身份出道，也在綜合節目中表現優秀，並監製直播劇《網紅崛起》找來好友陳為民、林柏宏力挺拍攝。 3. KO9蔡一零（黃鴻升飾）：雖然只有短暫的客串，但他以蓮花指和嬌滾嗓音，瞬間抓住觀眾的心。然而，他在2020年猝逕於家中離世。 4. KO8蔡五熊（ 蔣頤榛飾）：雖然最初是漂亮的美少女形象，但她挑戰扮演了個性十足的人物，並以「 五 熊」之名打開知識度。目前已經結婚並育有兩人。 5. KO7蔡雲寒（ 蔣函岑飾）：雖然在劇中展現出冷酷霸氣的一面，但她對妹姊五熊卻充滿溫柔關愛，反差萌成為角色亮點之一。目前已經宣布即將結婚喜訊。 6. KO136鯊魚（謝 和弦飾）：在劇中是金寶三的好友，並以歌手身份出道至今仍持續創作音樂，也與第二任妻子陳緗妮舉辦盛大婚禮並育有兩名女兒「音悅」和 「 音芙 」，事業家庭雙豐收。 7. KO13煞姐 （黃小柔 飾） : 依然活躍於演藝圈，不僅以歌手身份出道，也在綜合節目中表現優秀，並監製直播劇《網紅崛起》找來\n",
            "62 Linux kernel 曾使用的 process scheduler --- completely fair.scheduler (CFS) 採用了红黑树（Red-Black Tree）來儲存排程相關資訊。\n",
            "63 霸王行动（Operation Overlord）\n",
            "64 《Cytus II》遊戲中「Body Talk」是由Tomoaki Hasegawa創作的歌曲，該角色為Sakurako。\n",
            "65 李琳山教授在國立臺灣大學所開設的信號與系統課程，在期末考前後會有一次演講，該學生們稱之為 \"死亡論\"。\n",
            "66 我无法提供相关信息，因为你没有给出完整的内容。\n",
            "67 冠軍為中華台北。\n",
            "68 中国四大奇书是《水浒传》、《三国演义》，以及后来取代的金瓶梅和西游记\n",
            "69 子时是中国传统的十二个时间段之一，相当于现在24小时制中的23:00至01：59。\n",
            "70 避免要錯過時限來完成作業的排程演算法稱為「即期任務優先排序」或是簡單地叫做 \"Real-Time Scheduling\"。\n",
            "71 該代號「C8763」在原作中明確對應的是星光連流擊（StarburstStream），也就是桐人所使用的招式。\n",
            "72 《斯卡羅》是一部改編自小說「傀儡花」的電視劇，描述了1867年罗妹号事件及南岬之盟的歷史故事。剧情背景起点在当年的3月，在台湾恒春半岛琅峤地区（今恆 春镇）遭遇暴风而触礁搂浅海边的一艘美国商船羅姐號，13人上岸求生时因误闯原住民族斯卡罗的领地，被当局认为侵略者被杀害。\n",
            "73 Google Colab 的订阅制中，若要使用 A100 高级 GPU，您需要訂閱「Colaboratory Pro+」。\n",
            "74 我無法在Google中找到相關資訊。\n",
            "75 根據提供的資訊，雪江同學想知道如果他要修足够学分不用簽減免申请书至少需要多少个课程。  台大大學部一般每年秋季選課時，每位新生都可以開放 15-25 學期（約為10~17 個單元）的空間，若有申請學費抵稅或其他相關優惠的話，這個數字可能會增加。\n",
            "76 我没有找到相关信息。\n",
            "77 劫持愛蜜莉雅並想取其為妻的人是菲魯特。\n",
            "78 《海綿寶宝》的主角是史努比。\n",
            "79 玉米是单子叶植物。\n",
            "80 中華民國陸軍，隸屬於国防部陆军司令部门兵力为三 quân之最。中华人民共和国（这里指的是台湾） 中华 民共和国 陇 军 軍歌 前六字 为：\"忠義無悔,勇冠天下\".\n",
            "81 我無法找到相關的資訊。\n",
            "82 憂傷湖（Lacus Doloris）、死lake （不確定是否有此名稱），忘記Lake  ，恐怖lakes   以及愛湾 (Sinus Amoris)。\n",
            "83 《C♯小调第14號鋼琴奏鳴曲》是貝多芬所創作的著名钢音作品，其較為人知的是其流行别称“月光套鸣”，而非它本身。\n",
            "84 無法從提供的相關信息中確定阿美聲樂團是由哪位歌手所舉辦。\n",
            "85 黏土人HuggyWuggys\n",
            "86 太麻里鄉屬於台東縣。\n",
            "87 米开朗基罗的《大卫》雕塑是意大利文艺复兴时期的一座著名艺术作品，创作于1501-04年。它是一尊高5.17 米的大理石男性裸体像，是佛羅倫薩美術學院收藏品之一。  米开朗基罗在《大卫》雕塑中表现了一个正在准备战斗的英雄形象，大力和勇敢是其主要特征。大部分艺术家都认为，创作这尊巨型的大理石人像，是一项极大的挑战。经过4年的努力，这座高3.96米、连基准高度5 米的人物雕塑终于完成了。  在《大卫》中，我们可以看到许多细节的描绘，如静脉突出，肌肉紧张等。这是因为艺术家对人体结构有着深入了解。同时，这尊巨型的大理石像也表现出了米开朗基罗独特的人物雕塑风格，即使女性人物，也呈现出的健美的身体。  《大卫》最初被认为是不成比例，因为它的小生殖器。但是，根据历史背景和社会环境限制，以及古希腊文化对人体形象描绘方式，这些解释都有道理。\n",
            "88 除了蔣中正之外，另一位曾短暫晉升特級上將（Five-star General）的将领是何宗義。\n",
            "89 2012年第二賽季世界大赛的总冠军是SKT。\n",
            "90 日本麻將非莊家一開始的手牌有 14 張。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "TuX1sV5FRmJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This agent is the core component that answers the question.\n",
        "# test performance by 1 qa_agent\n",
        "qa_agent_test = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n",
        "    task_description=\"請回答以下問題：\",\n",
        ")\n",
        "\n",
        "async def pipeline_test1(question: str) -> str:\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
        "\n",
        "    # Step 1: 问题清理\n",
        "\n",
        "    initial_answer = qa_agent_test.inference(question);\n",
        "    #print(initial_answer)\n",
        "\n",
        "    # Step 5: 答案质量检查;\n",
        "    #final_answer = answer_verification_agent.inference(initial_answer);\n",
        "    #print(final_answer)\n",
        "\n",
        "    #return final_answer\n",
        "    return initial_answer\n",
        "\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID_test = \"li4agent_answer55\"\n",
        "\n",
        "STUDENT_ID_test = STUDENT_ID_test.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID_test}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline_test1(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID_test}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID_test}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline_test1(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID_test}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzS6nHUi41jV",
        "outputId": "68649bd1-5d99-4076-cabb-50c893d35692"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"虎山雄風飛揚 \" 是國立臺灣師範大學的校歌。\n",
            "2 對不起，我無法提供關於2025年初NCC規定的具體數字。\n",
            "3 第一代 iPhone 是由史蒂夫·乔布斯（Steve Jobs）發表的。\n",
            "4 根據台灣大學的進階英文免修申請規定，托福網路測驗 TOEFL iBT 的成績需達到 80 分以上才能符合資格。\n",
            "5 在橄欖球聯盟（Rugby Union）中，觸地試踢得 5 分。\n",
            "6 根據歷史記載，卑南族的祖先發源自ruvuwa'an，這個地點被認為是位於現今臺灣省臺東縣太麻里鄉附近。\n",
            "7 根據我的知識，熊仔的碩士指導教授是黃韻玲。\n",
            "8 答案：詹姆斯·克拉ーク・マ克斯韋爾。\n",
            "9 距離國立臺灣史前文化博物館最近的臺鐵車站是台東火车駅。\n",
            "10 答案是：50\n",
            "11 對不起，我無法提供關於未來的 NBA 賽事資訊，因為我只知道到 2023 年底。\n",
            "12 對不起，我無法提供關於未來事件的確切信息，特別是政治選舉結果。2024年美國總統大选尚在進行中，並且各個候任人士仍然競爭當時。我可以幫助你了解更多有关于美国总统竞争和历史上相关事項嗎？\n",
            "13 根據我的知識，Llama-3.1 系列模型中參數量最小的為 LLaMA 7B（約有七十億個参数），但我無法確認是否能找到更低版本。\n",
            "14 根據國立臺灣大學學則，停修有更嚴格的限制。依照規定，在沒有師生雙方同意的情況下，每個班級每年最多只能申請兩門課程停止上课（即「退選」），但如果是因為個人原因需要暫時休息或轉學等情形，則可以向學校提出停修申请。\n",
            "15 對不起，我無法找到任何有關DeepSeek公司的相關資訊。\n",
            "16 對不起，我無法提供最新的資訊或未來事件。2024年NBA總冠軍隊伍尚待確認，請關注官方新聞和賽事結果以獲得最準確且更新至今的情況報告\n",
            "17 這類化合物稱為烯（Alkene）。\n",
            "18 答案是阿倫·圖靈（Alan Turing）。他是一位英國數學家、計算機科學院院士和密碼破譯者，被認為是在20世紀中期對現代電腦理論做出重大貢獻的人。他的「图灵机」概念奠定了计算理论的基礎，並且他提出的圖靈測試也成為人工智能研究中的重要指標之一。他在二戰期間參與密碼破譯工作，對於德國的情报機構Enigma進行分析和攻擊做出了重大貢獻。\n",
            "19 根據我的知識，臺灣玄天上帝信仰的進香中心位於新北市瑞芳區。\n",
            "20 Windows 作業系統是微軟公司（Microsoft）的產品。\n",
            "21 官將首的起源是由於臺灣南部某間廟宇創立，該名為「天后宮」的福建移民在清治時期所建立。\n",
            "22 《咒》的邪神名為 \"阿修羅\"。\n",
            "23 這個問題的答案是：短暫交會的是五月天的一首歌曲，「旅程就此分岔」就是那一句經典台語詞。\n",
            "24 對不起，我無法提供 2025 年卑南族聯合年聚的具體資訊，因為這類型的情況可能會根據當地部落和組織者的安排而有所變動。然而，通常來說，這些活動是由各個原住民族委員会或相關機構負責籌辦。  如果你想知道 2025 年卑南族聯合年聚的具體資訊，我建議您可以查詢以下幾種途徃：  1. 卫生部卫生福利厅：他們可能會有關於原住民族活動和節慶相關信息。 2.Taiwan Indigenous Peoples' Festival Committee（臺灣土著人民文化藝術祭籌備委員会）：這是一個促進台灣各族群間的交流與合作組織，很大機率能夠提供你想要知道的情況。\n",
            "25 最新的輝達顯卡系列是「GeForce RTX 40 系列」。這個新一代產品使用了 NVIDIA 的 Ada Lovelace 架構，提供更高效能、更多功能和改進的人工智慧能力。\n",
            "26 大S（原名陳嘉樺）是在2013年8月去世的，她在美國洛杉磯因為肝硬化併發症而逕自離開醫院，後來被送到急診室，但已經無法挽回。\n",
            "27 萬有引力的發現通常被歸功於英國物理學家艾薩克·牛頓。他在1687年出版的著作《哲理自然科論》中提出三大運動定律和万有的吸力法則，描述了物體之間彼此相互作用並受到萬有引力的影響。\n",
            "28 台鵠開示計畫「TAIHUCAIS」的英文全名為 \"Taiwan Indigenous Cultural Assets Information System\"。\n",
            "29 「I'll be back」是出自1984年電影《終極追殺令》（The Terminator）的經典台詞，由阿諾·史瓦辛格飾演的角色泰倫森（T-800）說出的。這句話已成為科幻片中的代表性標誌之一，常被引用或模仿在其他作品中出現。在《終極追殺令》系列電影和許多衍生品裡，這一台詞都有重覆登場，並且變成了一種文化符號，用來表示復仇、報 thù等意義。\n",
            "30 水的化學式是 H2O。\n",
            "31 對不起，我無法提供李宏毅教授在《機器學習》2023年春季班中第15個作業的名稱。\n",
            "32 根據我的知識，目前臺灣公立的獨립學院僅剩一間是國防管理学院（簡稱：德明科大）。\n",
            "33 BitTorrent 協議使用的機制是叫做 \"Tracker\" 和 \"_piece_選擇算法（Piece Selection Algorithm）或稱為 _bitfield_\"。  當一個新的節點加入網路時，該協定會要求 Tracker 服務器提供一份包含所有種子的資訊清單。這些信息包括每個种子拥有的块的数量和哪个block已经下载完成等数据。这使得新节点可以知道谁有它需要什么。  接下来的步骤是使用 _bitfield_ 機制，該機製會將節點所缺乏資料分成多塊，並根據每一片是否已經被其他種子擁有的情況來決定哪些部分的資訊可以從那個特定的种子的哪里下载。\n",
            "34 這個描述聽起來很熟悉！我懷疑你正在說的是一段名為 \"Go Back in Time\" 的影片，內容是兩個人在路上遇到了一場意外車禍。騎摩托的那人用英文跟另外一個男人要求他不要往前走（go back），但太晚了，他們就出現事故。  這個短篇故事被拍成一段動畫影片，內容很簡單，但卻傳達了一種預防意外發生的重要性。\n",
            "35 根據研究人員的觀察報告和實驗結果，戈芬氏鳳頭鸚鵡最受偏好的乳酪口味是什麼呢？答案是在這個文中並沒有明確提及。然而，我們可以從文章中的描述推測出一些線索。  在整篇報導當 中，都有強調戈芬氏鳳頭鸚鵡對於某一特定乳酪口味的偏好，並且這種行為似乎是由它们認識到經過浸泡與翻轉後，食物變得更加美妙而導致。然而，這個文中並沒有明確指出哪一個奶油醬料最受鸚鵡喜愛。  但根據實驗結果，我們可以推測這些戈芬氏鳳頭鳥可能對於一些較為濃郁、香味浓烈的乳酪口感有著特別偏好的傾向。因爲在文中提到，這種行徑似乎是由它们認識到了經過處理後，食物變得更加美妙而導致。  因此，我們可以推測這些戈芬氏鳳頭鳥最受喜愛的乳酪口味可能包括一些較為濃郁、香氣浓烈或是特殊風格的情況。\n",
            "36 很抱歉，我無法找到相關的資訊，關於2024年桃園Xpark水族館國王企鵝「嘟胖」和 「烏龍茶」的產下一隻小孩。\n",
            "37 根據國立臺灣大學的資訊，物理治療學系目前正常修業年限為四年的碩士班和三年半至五個月（含實習）的博士後研究生。\n",
            "38 《BanG Dream!》中，「呼嘿哈」或是 「呵～咯」的笑聲習慣，是由Riko Kawamura所扮演的角色。\n",
            "39 甲斐之虎是日本戰國時代的一位著名武將，他就是德川家康的前任領主——高坂昌氏（後來改姓為松平），但更常被稱呼的是他的兒子，亦即「真田幸村」之前在信濃(甲斐)地區統治時期所獲得之別名：\"武藏野虎\"(又譯作 \"山中鹿主\")的前任領地——高坂昌氏後裔中的松平家的一員:  高城親長。\n",
            "40 根據王肥貓同學的標準，他想要選擇網路上最多好評的一門課。因此，我們需要找出這三個候補中的哪一堂是台大生中人氣最高、口碑最佳。  雖然我沒有實時存取最新資訊，但根據過往資料，「數位素養導航」是一道非常受歡迎的通識課程。它教學內容豐富且有趣，並結合了現代社會中的科技與生活應用，因此很可能是王肥貓同事最喜愛的一堂。  因此，我們可以推測，依照網路評價和口碑，他選擇的課程大概會是我所說這一門。\n",
            "41 對不起，我無法提供2024年的第42回《極限體能王SASUKE》首播日期的資訊，因為我沒有最新或實時更新的情況。然而，根據過往慣例和節目安排，你可以嘗試在日本TBS電視台官方網站、社交媒介平台，或是其他相關新聞來源查詢2024年的第42回《極限體能王SASUKE》首播日期的最新資訊。\n",
            "42 根據歷史記載，出身於利嘉部落後來成為初鹿（或稱為大隘）頭目的漢人，是名叫「張永春」的。\n",
            "43 《BanG Dream! Ave Mujica》的片頭曲是「」。\n",
            "44 Linux作業系統最早於1991年首次發布。由林納斯·托瓦茲（Linus Torvalds）在芬蘭赫爾辛基大學開源發展的 Linux核心，最初是為了取代MINIX而設計的一個Unix-like操作系统，並且迅速演變成一個完整、可用的作業系統。在1991年9月25日，他發布了一份名叫\"Linux 0.01”的原始碼。\n",
            "45 根據你的描述，Likavung 部落的中文名稱應該是「利卡武崗部族」或簡稱為 \" 利克\"\n",
            "46 紅茶是一種半發酵或部分生抽的黑tea類型。\n",
            "47 在《遊戲王》卡牌中，以「真紅眼黑龍」與 「黒魔導師」的融合素材，能夠創造出名為 \"Black Luster Soldier of Destruction\" 的強力怪獸。\n",
            "48 豐田萌繪在《BanG Dream!》企劃中，擔任Poppin'Party的角色聲優——櫻Innercircle（咲坂伊澄）。\n",
            "49 Rugby Union 中，9 號球員的正式名稱為「掃邊後衛」（Scrum-half）。\n",
            "50 你問的問題是關於太陽系中的矮行星。根據最新的地球科學研究，曾被視為第八顆大氣體巨蟒（或稱天王）但最終降格成小型冰霸主、又名冥衛一之外，最早的也是最著明的是普羅米修斯和海倫，但更常見且有爭議性的答案是：   那就是我們熟悉的小行星1號， Pluto（布魯頓）。\n",
            "51 根據我的知識，臺灣最早成立的野生動物救傷單位是位於台北市內湖區的大愛鷹醫院。\n",
            "52 根據我的知識，特生中心在2023年改名後的名字是「集結自然館」。\n",
            "53 根據我的知識庫，Developing Instruction-FollowING Speech Language Model Without speech instruction-tuning data論文中提出的模型是名為\"Instruction-Tuned (IT) 模型的替代方案，即 \"Zero-Shot IT-Free（ZS-IF）\"\"\n",
            "54 太陽系中體積最大的行星是土壤。\n",
            "55 根據語言分類學的研究，臺灣原住民族中的阿美族（Amis）和卑南人等部落使用屬於馬來-波利尼西亞支系下的達悟文。然而，這些與其他台灣本土之島嶼上的多數原始民間所說語言相比，其分類學上更接近菲律賓群岛的阿塔伊族（Atayal）等部落使用的一種稱為「南亞-太平洋」或是馬來波利尼西亚支系下的泰雅文。\n",
            "56 很抱歉，我無法找到相關的資訊或證據來確認這句話是誰說出過。然而，這個故事可能源自於一位台灣大學老師，許多人猜測他就是陳景峻教授，但我找不到任何可靠的情況可以支持此事實。  如果你有更多關係的資訊或上下文，我會很樂意幫助您進一步探索。\n",
            "57 「embiyax namu kana」是阿美族的打招呼用語。\n",
            "58 根據我的知識，「鄒與布農, 永久美麗」這句話是指位於台東縣的達仁鄉的一個部落——大武山。該地區因為地理位置特殊，被認爲具有豐富文化資產和自然景觀，因此被稱作永久之都。  在歷史上，大部分鄒族人與布農人的混居區域都是位於台東縣的達仁鄉，這裡是兩個民族交融的地方。因此，「 鄄 與 布 紅, 永久美麗」這句話很可能是在描述大武山地區或附近部落的情況。  然而，我無法確定哪一個具體的地點與該詞語最相關，因為我沒有找到足夠的資訊來支持任何特定的結論。\n",
            "59 很抱歉，我無法找到相關的資訊。\n",
            "60 根據卑南族的傳說，姊弟 Tuku 及 Sihasihau 分別創建了 Amis 和 Paiwan 兩個部落。\n",
            "61 《終極一班》中的「KO榜」是高中生戰力排行的名單，該劇中提到 KO 排在第 1 位的是 \"阿飛\"。\n",
            "62 Linux kernel 中的 Completely Fair Scheduler (CFS) 使用紅黑樹（Red-Black Tree）來儲存排程相關資訊。這種資料結構能夠有效地維護和查找各個進程序所在位置，從而實現公平且高效率的情況下進行任務的調度。  CFS 會根據每一個 task 的 nice 值、執行時間等因素來計算其優先順序，並將這些資訊儲存於紅黑樹中。當需要選擇哪個進程要被運算時，系統就可以通過查找和比較各項資料結構中的值，以確保每一個 task 都能夠得到公平的執行時間。  使用红.black树作为task调度器的一个优点是，它能够有效地维护任务之间的一致性，并且在插入或删除一个新节点时，保持了二叉查找数（BST）的属性。\n",
            "63 諾曼第登陸（Normandy Landings）的作戰代號為「奧運會」（Operation Overlord）。\n",
            "64 《Cytus II》遊戲中「Body Talk」是由Tomoaki Watanabe的角色所唱。\n",
            "65 李琳山教授開設的信號與系統課程，期末考前後的一次演講被稱為「最後一堂上午」或簡單地叫做 \" 上學時\"\n",
            "66 根據 NVIDIA 的官方資訊，RTX 5090 顯卡的 VRAM（視覺記憶體）容量為24GB GDDR6。這比前代 RTXTM40903030顯示核心更大，並且提供了更多空間來部署 LLM 等應用程式。  值得注意的是，RTX 5090 顯卡的 VRAM 容acity 不僅是指物理記憶體容量，也包括 GPU 的內存管理和效率優化。這意味著即使在相同大小的情況下，這款顯示核心也能更有效地利用VR AM資源來提升整体性能。  因此，根據 NVIDIA 官方的資料，我們可以確認 RTX 5090 顯卡確實具有24GB GDDR6 的 VRAM容量。\n",
            "67 對不起，我無法提供2024年世界棒球12強賽的冠軍隊伍資訊，因為我沒有最新或即時更新至 26 Jul 的資料。\n",
            "68 中國四大奇書是指《西遊記》、《水滸傳》，以及兩本古典小說： 《三國演義》（又稱為「金瓶梅」並非其中之一）和『 Romance of the Three Kingdoms』的原名不是這樣，實際上應該叫做 '_ 三俠五傑十美oni' 的前身《水滸傳》與後來的小說改編而成 '三國演義'_。\n",
            "69 子時是中國傳統的十二個時間段之一，對應於西曆中的凌晨1點至3点。\n",
            "70 在作業系統中，避免要錯過時限來完成任務的排程演算法稱為「即期式」或是\"Real-time Scheduling\"(RTS)。這類型的手段會根據每個程序或者工作單位所需執行時間以及優先順序等因素，確保在指定之時限內完成任務。  然而，在一般的作業系統中，更常見的是使用「非即期式」或是\"Non-Real-time Scheduling\"(NRTS) 的排程演算法，如FCFS（First-Come, First-Served）、SJF(Shortest Job Fist)、SRTN(Sporadic Server Task Model)，等。這類型的手段不一定能夠確保任務在指定之時限內完成，但可以根據系統的整體效率和資源分配來進行優化。  需要注意的是，即期式排程演算法通常會對實際應用有較高要求，例如醫療設備、飛機控制等。\n",
            "71 在《刀劍神域》中，招式代號「C8763」是由Saki Nakamura持有的。\n",
            "72 《斯卡羅》是一部以美國西南邊境為背景的歷史小說，描述了墨日戰爭時期的一個故事。劇中之地名「柴城」位於現今新罕布什爾州（New Hampshire）的北方地區。  然而，我們知道這裡其實是指的是現在屬於美國麻薩諸塞省的西部邊界，特別是在當年被稱為 \"Cheshire County\" 的地方。\n",
            "73 根據Google Colab的最新資訊，若要使用A100高級GPU，您需要訂閱「Colabs Pro+」。\n",
            "74 李宏毅老師開設的機器學習課程是屬於台大資訊工程學院（College of Electrical Engineering and Computer Science）的電腦科系。\n",
            "75 根據國立臺灣大學的規定，學生每年修滿一定數量的心力科目（一般為 24 學分）才能獲得書卷獎。假設雪江同事想要避免簽減課申請表，他至少需要在本年度內完成多少個心力的選擇性或必須的主題。  根據國立臺灣大學資工系的心力科目規定，學生每年修滿 24 學分即可獲得書卷獎。因此，如果雪江同事想要避免簽減課申請表，他至少需要在本年度內完成不少於 **12** 個心力的選擇性或必須的主題（假設一科學生每年修滿 24 學分即可獲得書卷獎）。  然而，雪江同事是一位大三級生的資工系畢業班，他可能需要額外完成一些課程來達到所需的心力數量。因此，我們建議他至少要在本年度內選擇不少於 **15** 個心力的主題或必須的科目，以確保能夠獲得書卷獎並避免簽減修學分申請表。  最後，雪江同事應該根據自己的需求和能力來決定是否要多加努力完成額外的心力課程。\n",
            "76 Neuro-sama 的最初 Live2D 模型是使用 VTube Studio 預設角色 \"VTuber\"。\n",
            "77 在「從零開始的異世界生活 第三季」動畫中，劫持愛蜜莉雅並想取其為妻的人是雷伊。\n",
            "78 《海綿寶宝》的主角在第五季的劇集中，擊敗刺破泡沫紅眼幇是在布魯克林市。\n",
            "79 玉米是一種雙子葉植物。\n",
            "80 中華民國陸軍的前六字為：「忠誠勇毅，義無反顧」。\n",
            "81 根據台大電資學院的規定，計算機科學與情報工程系（CSIE）是其中一個例外。這個系統允許選修一門自然 科目的課程，而不是必須全部三堂。  然而，我們需要注意的是，這些信息可能會隨著時間變化而更新，因此建議您查詢最新的資訊或與台大電財學院聯繫以確認具體規定。\n",
            "82 憂傷湖（Lacus Doloris）、死lake （不確定是不是 Lacus Morti s） 、忘海 ( 不一定 是 lacu oblivi on is )、恐怖 lake  和愛灣都在月球的背面。\n",
            "83 《C♯小調第14號鋼琴奏鳴曲》是貝多芬所創作的作品，其較為人知的是「月光鈴」。\n",
            "84 阿米斯音樂節（Coachella Valley Music and Arts Festival）是一個國際知名的音乐节，於1999年由保羅·費爾德施耐徹和亞倫‧萊文所創辦。\n",
            "85 在「Poppy Playtime - Chapter 4」遊戲中，黏土人被稱為 \"Huggy Wugs\"。\n",
            "86 根據你的問題，賓茂村其實屬於屏東縣的里港鄉。\n",
            "87 米開朗基羅的《大衛》雕像最初是在佛罗伦萨的一個花崗岩石坑中被創作出來。這座著名的大理壇作品於1501年至1520年代間完成，並在1512-1564 年期間展覽，之後移到米蘭的聖母院（Duomo），並一直留存到1796年的拿破崙時期才被運回佛羅倫薩。\n",
            "88 根據中華民國軍事史，除了蔣介石外，一位曾短暫晉升特級上將的将领是嚴家淦。\n",
            "89 2012年英雄聯盟世界大賽的總冠軍是韓國戰隊「SK Telecom T1」。\n",
            "90 在日本麻將中，非莊家一開始的手牌通常有14張。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID_test}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID_test}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "WjKwqhw6RpQZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}